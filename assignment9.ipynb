{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 챌린지\n",
    "---\n",
    "(EN)\n",
    "- In a new Jupyter notebook create a research AI agent and give it custom tools.\n",
    "- The agent should be able to do the following tasks:\n",
    "    - Search in Wikipedia\n",
    "    - Search in DuckDuckGo\n",
    "    - Scrape and extract the text of any website.\n",
    "    - Save the research to a .txt file\n",
    "- Run the agent with this query: \"Research about the XZ backdoor\", the agent should try to search in Wikipedia or DuckDuckGo, if it finds a website in DuckDuckGo it should enter the website and extract it's content, then it should finish by saving the research to a .txt file.\n",
    "\n",
    "(KR)\n",
    "- 새로운 Jupyter notebook에서 리서치 AI 에이전트를 만들고 커스텀 도구를 부여합니다.\n",
    "- 에이전트는 다음 작업을 수행할 수 있어야 합니다:\n",
    "    - Wikipedia에서 검색\n",
    "    - DuckDuckGo에서 검색\n",
    "    - 웹사이트의 텍스트를 스크랩하고 추출합니다.\n",
    "    - 리서치 결과를 .txt 파일에 저장하기\n",
    "- 다음 쿼리로 에이전트를 실행합니다: \"Research about the XZ backdoor\" 라는 쿼리로 에이전트를 실행하면, 에이전트는 Wikipedia 또는 DuckDuckGo에서 검색을 시도하고, DuckDuckGo에서 웹사이트를 찾으면 해당 웹사이트에 들어가서 콘텐츠를 추출한 다음 .txt 파일에 조사 내용을 저장하는 것으로 완료해야 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import wikipedia\n",
    "import requests\n",
    "from duckduckgo_search import DDGS\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define AI agent tools\n",
    "def search_wikipedia(query):\n",
    "    try:\n",
    "        summary = wikipedia.summary(query, sentences=5)\n",
    "        return f\"Wikipedia Summary:\\n{summary}\"\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        return f\"Wikipedia Disambiguation Error: {e.options[:5]}\"\n",
    "    except wikipedia.exceptions.PageError:\n",
    "        return \"Wikipedia Page not found.\"\n",
    "\n",
    "def search_duckduckgo(query):\n",
    "    with DDGS() as ddgs:\n",
    "        results = list(ddgs.text(query, max_results=5))\n",
    "    if results:\n",
    "        return [result[\"href\"] for result in results if \"href\" in result]\n",
    "    return []\n",
    "\n",
    "def scrape_website(url):\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            paragraphs = soup.find_all('p')\n",
    "            text = '\\n'.join([p.get_text() for p in paragraphs])\n",
    "            return text[:2000]  # Limit extracted text for readability\n",
    "        else:\n",
    "            return \"Failed to fetch the website.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error scraping website: {str(e)}\"\n",
    "\n",
    "def save_research_to_file(research_text, filename=\"research.txt\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(research_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI Agent Execution\n",
    "def research_ai_agent(query):\n",
    "    research_results = []\n",
    "    \n",
    "    # Step 1: Search Wikipedia\n",
    "    wiki_result = search_wikipedia(query)\n",
    "    research_results.append(wiki_result)\n",
    "    \n",
    "    # Step 2: Search DuckDuckGo\n",
    "    urls = search_duckduckgo(query)\n",
    "    if urls:\n",
    "        research_results.append(\"\\nDuckDuckGo Found URLs:\\n\" + \"\\n\".join(urls))\n",
    "        \n",
    "        # Step 3: Scrape first relevant URL\n",
    "        scraped_text = scrape_website(urls[0])\n",
    "        research_results.append(\"\\nExtracted Website Content:\\n\" + scraped_text)\n",
    "    \n",
    "    # Step 4: Save research to file\n",
    "    final_research = '\\n\\n'.join(research_results)\n",
    "    save_research_to_file(final_research)\n",
    "    \n",
    "    return final_research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia Summary:\n",
      "In February 2024, a malicious backdoor was introduced to the Linux build of the xz utility within the liblzma library in versions 5.6.0 and 5.6.1 by an account using the name \"Jia Tan\". The backdoor gives an attacker who possesses a specific Ed448 private key remote code execution through OpenSSH on the affected Linux system. The issue has been given the Common Vulnerabilities and Exposures number CVE-2024-3094 and has been assigned a CVSS score of 10.0, the highest possible score.\n",
      "While xz is commonly present in most Linux distributions, at the time of discovery the backdoored version had not yet been widely deployed to production systems, but was present in development versions of major distributions. The backdoor was discovered by the software developer Andres Freund, who announced his findings on 29 March 2024.\n",
      "\n",
      "\n",
      "DuckDuckGo Found URLs:\n",
      "https://research.swtch.com/xz-timeline\n",
      "https://boehs.org/node/everything-i-know-about-the-xz-backdoor\n",
      "https://en.wikipedia.org/wiki/XZ_Utils_backdoor\n",
      "https://www.wired.com/story/xz-backdoor-everything-you-need-to-know/\n",
      "https://jfrog.com/blog/xz-backdoor-attack-cve-2024-3094-all-you-need-to-know/\n",
      "\n",
      "\n",
      "Extracted Website Content:\n",
      "\n",
      "Over a period of over two years, an attacker using the name “Jia Tan”\n",
      "worked as a diligent, effective contributor to the xz compression library,\n",
      "eventually being granted commit access and maintainership.\n",
      "Using that access, they installed a very subtle, carefully hidden backdoor into liblzma,\n",
      "a part of xz that also happens to be a dependency of OpenSSH sshd\n",
      "on Debian, Ubuntu, and Fedora, and other systemd-based Linux systems that patched sshd to link libsystemd.\n",
      "(Note that this does not include systems like Arch Linux, Gentoo, and NixOS, which do not patch sshd.)\n",
      "That backdoor watches for the attacker sending hidden commands at the start of an SSH session,\n",
      "giving the attacker the ability to run an arbitrary command on the target system without logging in:\n",
      "unauthenticated, targeted remote code execution.\n",
      "\n",
      "\n",
      "The attack was publicly disclosed on March 29, 2024 and\n",
      "appears to be the first serious known supply chain attack on widely used open source software.\n",
      "It marks a watershed moment in open source supply chain security, for better or worse.\n",
      "\n",
      "\n",
      "This post is a detailed timeline that I have constructed of the\n",
      "social engineering aspect of the attack, which appears to date\n",
      "back to late 2021.\n",
      "(See also my analysis of the attack script.)\n",
      "\n",
      "\n",
      "Corrections or additions welcome on Bluesky, Mastodon, or email.\n",
      "Prologue\n",
      "\n",
      "2005–2008: Lasse Collin, with help from others, designs the .xz file format using the LZMA compression algorithm, which compresses files to about 70% of what gzip did [1]. Over time this format becomes widely used for compressing tar files, Linux kernel images, and many other uses.\n",
      "Jia Tan arrives on scene, with supporting cast\n",
      "\n",
      "2021-10-29: Jia Tan sends first, innocuous patch to the xz-devel mailing list, adding “.editorconfig” file.\n",
      "\n",
      "\n",
      "2021-11-29: Jia Tan sends second innocuous patch to the xz-devel mailing list, fixing an apparent reproducible build problem. More patches that seem (even in retrospect) to be fine follow.\n",
      "\n",
      "\n",
      "2022-02-07: Lasse Collin merges first commi\n"
     ]
    }
   ],
   "source": [
    "# Run the agent with the query \"Research about the XZ backdoor\"\n",
    "query = \"Research about the XZ backdoor\"\n",
    "research_result = research_ai_agent(query)\n",
    "print(research_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPTChallenge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
